{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LearningRotations_6D.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP8w+W/cdpqWTRGC/WFxeJa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/albertomariapepe/Learning-Rotations/blob/main/LearningRotations_6D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "525_Em9E0nxc"
      },
      "source": [
        "!pip install git+https://github.com/pygae/clifford.git@master"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHCD47lEXWMy"
      },
      "source": [
        "!pip install tensorflow_graphics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3aVTpc0S_6hG",
        "outputId": "77f2afbc-3a17-4722-e10c-b201c60944a9"
      },
      "source": [
        "from scipy.spatial.transform import Rotation as R\n",
        "import numpy as np\n",
        "from clifford.g3c import *\n",
        "from clifford.tools.g3c import *\n",
        "from clifford.tools.g3c.rotor_parameterisation import *\n",
        "from tensorflow.keras.layers import Input, Dense, LeakyReLU, Dropout\n",
        "from keras import backend as K\n",
        "from sklearn.model_selection import train_test_split\n",
        "from math import acos\n",
        "import tensorflow as tf\n",
        "from matplotlib import pyplot as plt\n",
        "import keras\n",
        "from math import e, pi\n",
        "import tensorflow_graphics as tfg\n",
        "import tensorflow_graphics.geometry.transformation as tfg_transformation\n",
        "\n",
        "#THESE TWO ARE REQUIRED TO CONVERT TENSOR TO NUMPY ARRAY\n",
        "#tf.compat.v1.enable_eager_execution()\n",
        "#tf.config.run_functions_eagerly(True)\n",
        "\n",
        "#GENERATING THE RANDOM DATASET\n",
        "tot = int(1e5)\n",
        "i = 0\n",
        "\n",
        "r = np.load('R.npy')\n",
        "r = np.reshape(r, [tot, 3, 3])  \n",
        "\n",
        "'''\n",
        "n = []\n",
        "for i in range(0,tot):\n",
        "    a1 = r[i,:,0]\n",
        "    a2 = r[i,:,1]\n",
        "\n",
        "    b1 = a1 / np.linalg.norm(a1)\n",
        "    b2 = a2 - np.dot(a1,b1)*b1\n",
        "    b2 = b2 / np.linalg.norm(b2)\n",
        "\n",
        "    b3 = np.cross(b1, b2)\n",
        "    #B1 = b1[0]*e1 + b1[1]*e2 + b1[2]*e3\n",
        "    #B2 = b2[0]*e1 + b2[1]*e2 + b2[2]*e3\n",
        "\n",
        "    #B3 = B1 ^ B2\n",
        "    #b3 = [B3[6], B3[7], B3[10]]\n",
        "    #b3 = b3/np.linalg.norm(b3)\n",
        "\n",
        "    n = np.append(n, np.transpose([b1, b2, b3]))\n",
        "    #n = np.append(n, np.transpose([b3]))\n",
        "\n",
        "'''\n",
        "r = np.reshape(r, [tot, 9])    \n",
        "\n",
        "#Train - Test Split\n",
        "r_train, r_test = train_test_split(r, test_size=0.33, shuffle=True)\n",
        "#n_train, n_test = train_test_split(n, test_size=0.33, shuffle=False)\n",
        "\n",
        "TRAIN = r_train\n",
        "TEST = r_test\n",
        "out_size = 9\n",
        "\n",
        "nb_epoch = 100\n",
        "batch_size = 128\n",
        "\n",
        "r_input = Input(shape=(9))\n",
        "x = Dense(128)(r_input)\n",
        "x = LeakyReLU(alpha=0.2)(x)\n",
        "x = Dense(128)(x)\n",
        "x = LeakyReLU(alpha=0.2)(x)\n",
        "x = Dense(128)(x)\n",
        "x = LeakyReLU(alpha=0.2)(x)\n",
        "out = Dense(out_size)(x)\n",
        "#out = LeakyReLU(alpha=0.2)(out)\n",
        "\n",
        "\n",
        "tf.executing_eagerly()\n",
        "\n",
        "\n",
        "def custom_loss(y_true, y_pred):\n",
        "\n",
        "    y_pred = tf.reshape(y_pred, [-1, 3, 3])\n",
        "  \n",
        "    a1 = y_pred[:,:,0]\n",
        "    a2 = y_pred[:,:,1]\n",
        "\n",
        "\n",
        "    b1, _ = tf.linalg.normalize(a1, ord='euclidean')\n",
        "    c = tf.tensordot(b1, a2, axes=[1, 1])\n",
        "    c = a2 - tf.matmul(c, b1)\n",
        "    b2, _ = tf.linalg.normalize(c, ord='euclidean')\n",
        "\n",
        "    b3 = tf.linalg.cross(b1, b2)\n",
        "    y_pred_rot = tf.concat([b1, b2, b3], axis = 1)\n",
        "\n",
        "    l2 = K.mean((y_true - y_pred_rot)**2)\n",
        "\n",
        "    return l2\n",
        "\n",
        "model = keras.Model(r_input,  out)\n",
        "model.summary()\n",
        "#opt = tf.keras.optimizers.SGD(learning_rate=0.1, momentum=0.9, nesterov=False)\n",
        "model.compile(loss=custom_loss, optimizer='adam')\n",
        "#es_callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=8)\n",
        "model_train = model.fit(x = r_train, y = r_train,\n",
        "                        validation_split=0.3,\n",
        "                        epochs=nb_epoch,\n",
        "                        shuffle = False,\n",
        "                        verbose=1,\n",
        "                        batch_size=batch_size)\n",
        "\n",
        "\n",
        "\n",
        "loss = model_train.history['loss']\n",
        "val_loss = model_train.history['val_loss']\n",
        "epochs = range(nb_epoch)\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, 'b-', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'r-', label='Validation loss')\n",
        "plt.title('Training loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "predicted = model.predict(r_test)\n",
        "\n",
        "M = []\n",
        "Langle = []\n",
        "\n",
        "for i in range(0,int(tot*0.33)):\n",
        "    y_pred = np.reshape(predicted[i], [3, 3])\n",
        "\n",
        "    a1 = y_pred[:,0]\n",
        "    a2 = y_pred[:,1]\n",
        "\n",
        "    b1 = a1 / np.linalg.norm(a1)\n",
        "    b2 = a2 - np.dot(a2,b1)*b1\n",
        "    b2 = b2 / np.linalg.norm(b2)\n",
        "\n",
        "    b3 = np.cross(b1, b2)\n",
        "\n",
        "    y_pred = np.transpose([b1, b2, b3])\n",
        "    y_pred = np.reshape([b1, b2, b3], [3,3])\n",
        "    y_real = np.reshape(r_test[i], [3, 3])\n",
        "\n",
        "    \n",
        "\n",
        "    M = np.matmul(y_real, np.linalg.inv(y_pred))\n",
        "    cosine = (M[0,0] + M[1,1] + M[2,2] - 1)/2\n",
        "\n",
        "    #M = np.dot(y_real,  1/y_pred)\n",
        "    #cosine = (M - 1)/2\n",
        "    if cosine > 1:\n",
        "        cosine = 1\n",
        "    \n",
        "    if cosine < -1:\n",
        "        cosine = -1\n",
        "    \n",
        "    #print(acos(cosine))\n",
        "    Langle = np.append(Langle, acos(cosine))\n",
        "\n",
        "\n",
        "\n",
        "print(np.max(Langle)*180/pi)\n",
        "print(np.average(Langle)*180/pi)\n",
        "print(np.std(Langle)*180/pi)\n",
        "\n",
        "#np.save('B_loss.npy', loss)\n",
        "np.save('sanity_6D_val_loss.npy', val_loss)\n",
        "#np.save('B_langle.npy', Langle)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_25\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "Total params: 35,465\n",
            "Trainable params: 35,465\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "367/367 [==============================] - 2s 4ms/step - loss: 0.2962 - val_loss: 0.2941\n",
            "Epoch 2/100\n",
            "367/367 [==============================] - 1s 4ms/step - loss: 0.2941 - val_loss: 0.2941\n",
            "Epoch 3/100\n",
            "367/367 [==============================] - 1s 4ms/step - loss: 0.2941 - val_loss: 0.2941\n",
            "Epoch 4/100\n",
            "367/367 [==============================] - 1s 4ms/step - loss: 0.2941 - val_loss: 0.2941\n",
            "Epoch 5/100\n",
            "367/367 [==============================] - 1s 4ms/step - loss: 0.2941 - val_loss: 0.2941\n",
            "Epoch 6/100\n",
            "367/367 [==============================] - 1s 4ms/step - loss: 0.2941 - val_loss: 0.2941\n",
            "Epoch 7/100\n",
            "367/367 [==============================] - 1s 4ms/step - loss: 0.2941 - val_loss: 0.2941\n",
            "Epoch 8/100\n",
            "367/367 [==============================] - 1s 4ms/step - loss: 0.2941 - val_loss: 0.2941\n",
            "Epoch 9/100\n",
            "367/367 [==============================] - 1s 3ms/step - loss: 0.2941 - val_loss: 0.2941\n",
            "Epoch 10/100\n",
            "367/367 [==============================] - 1s 3ms/step - loss: 0.2941 - val_loss: 0.2941\n",
            "Epoch 11/100\n",
            "367/367 [==============================] - 1s 3ms/step - loss: 0.2941 - val_loss: 0.2941\n",
            "Epoch 12/100\n",
            "367/367 [==============================] - 1s 4ms/step - loss: 0.2941 - val_loss: 0.2941\n",
            "Epoch 13/100\n",
            "367/367 [==============================] - 1s 4ms/step - loss: 0.2941 - val_loss: 0.2941\n",
            "Epoch 14/100\n",
            "367/367 [==============================] - 1s 4ms/step - loss: 0.2941 - val_loss: 0.2941\n",
            "Epoch 15/100\n",
            "367/367 [==============================] - 1s 4ms/step - loss: 0.2941 - val_loss: 0.2941\n",
            "Epoch 16/100\n",
            "367/367 [==============================] - 1s 3ms/step - loss: 0.2941 - val_loss: 0.2941\n",
            "Epoch 17/100\n",
            "367/367 [==============================] - 1s 4ms/step - loss: 0.2941 - val_loss: 0.2941\n",
            "Epoch 18/100\n",
            "367/367 [==============================] - 1s 4ms/step - loss: 0.2941 - val_loss: 0.2941\n",
            "Epoch 19/100\n",
            "367/367 [==============================] - 1s 4ms/step - loss: 0.2941 - val_loss: 0.2941\n",
            "Epoch 20/100\n",
            "367/367 [==============================] - 1s 4ms/step - loss: 0.2941 - val_loss: 0.2941\n",
            "Epoch 21/100\n",
            "367/367 [==============================] - 1s 4ms/step - loss: 0.2941 - val_loss: 0.2941\n",
            "Epoch 22/100\n",
            "367/367 [==============================] - 1s 4ms/step - loss: 0.2941 - val_loss: 0.2941\n",
            "Epoch 23/100\n",
            "367/367 [==============================] - 1s 4ms/step - loss: 0.2941 - val_loss: 0.2941\n",
            "Epoch 24/100\n",
            "367/367 [==============================] - 1s 4ms/step - loss: 0.2941 - val_loss: 0.2941\n",
            "Epoch 25/100\n",
            "367/367 [==============================] - 1s 4ms/step - loss: 0.2941 - val_loss: 0.2941\n",
            "Epoch 26/100\n",
            "367/367 [==============================] - 1s 4ms/step - loss: 0.2941 - val_loss: 0.2941\n",
            "Epoch 27/100\n",
            "367/367 [==============================] - 1s 4ms/step - loss: 0.2941 - val_loss: 0.2941\n",
            "Epoch 28/100\n",
            "367/367 [==============================] - 1s 4ms/step - loss: 0.2941 - val_loss: 0.2941\n",
            "Epoch 29/100\n",
            "367/367 [==============================] - 1s 4ms/step - loss: 0.2941 - val_loss: 0.2941\n",
            "Epoch 30/100\n",
            "367/367 [==============================] - 1s 4ms/step - loss: 0.2941 - val_loss: 0.2941\n",
            "Epoch 31/100\n",
            "367/367 [==============================] - 1s 4ms/step - loss: 0.2941 - val_loss: 0.2941\n",
            "Epoch 32/100\n",
            "367/367 [==============================] - 1s 4ms/step - loss: 0.2941 - val_loss: 0.2941\n",
            "Epoch 33/100\n",
            "367/367 [==============================] - 1s 4ms/step - loss: 0.2941 - val_loss: 0.2941\n",
            "Epoch 34/100\n",
            "367/367 [==============================] - 1s 4ms/step - loss: 0.2941 - val_loss: 0.2941\n",
            "Epoch 35/100\n",
            "367/367 [==============================] - 1s 4ms/step - loss: 0.2941 - val_loss: 0.2941\n",
            "Epoch 36/100\n",
            "367/367 [==============================] - 1s 4ms/step - loss: 0.2941 - val_loss: 0.2941\n",
            "Epoch 37/100\n",
            "367/367 [==============================] - 1s 4ms/step - loss: 0.2941 - val_loss: 0.2941\n",
            "Epoch 38/100\n",
            "367/367 [==============================] - 1s 4ms/step - loss: 0.2941 - val_loss: 0.2941\n",
            "Epoch 39/100\n",
            "367/367 [==============================] - 1s 3ms/step - loss: 0.2941 - val_loss: 0.2941\n",
            "Epoch 40/100\n",
            "367/367 [==============================] - 1s 4ms/step - loss: 0.2941 - val_loss: 0.2941\n",
            "Epoch 41/100\n",
            "367/367 [==============================] - 1s 4ms/step - loss: 0.2941 - val_loss: 0.2941\n",
            "Epoch 42/100\n",
            "367/367 [==============================] - 1s 4ms/step - loss: 0.2941 - val_loss: 0.2941\n",
            "Epoch 43/100\n",
            "367/367 [==============================] - 1s 4ms/step - loss: 0.2941 - val_loss: 0.2941\n",
            "Epoch 44/100\n",
            "367/367 [==============================] - 1s 3ms/step - loss: 0.2941 - val_loss: 0.2941\n",
            "Epoch 45/100\n",
            "367/367 [==============================] - 1s 3ms/step - loss: 0.2941 - val_loss: 0.2941\n",
            "Epoch 46/100\n",
            "367/367 [==============================] - 1s 3ms/step - loss: 0.2941 - val_loss: 0.2941\n",
            "Epoch 47/100\n",
            "367/367 [==============================] - 1s 4ms/step - loss: 0.2941 - val_loss: 0.2941\n",
            "Epoch 48/100\n",
            "367/367 [==============================] - 1s 3ms/step - loss: 0.2941 - val_loss: 0.2941\n",
            "Epoch 49/100\n",
            "367/367 [==============================] - 1s 3ms/step - loss: 0.2941 - val_loss: 0.2941\n",
            "Epoch 50/100\n",
            "367/367 [==============================] - 1s 4ms/step - loss: 0.2941 - val_loss: 0.2941\n",
            "Epoch 51/100\n",
            "367/367 [==============================] - 1s 4ms/step - loss: 0.2941 - val_loss: 0.2941\n",
            "Epoch 52/100\n",
            "367/367 [==============================] - 1s 4ms/step - loss: 0.2941 - val_loss: 0.2941\n",
            "Epoch 53/100\n",
            "367/367 [==============================] - 1s 3ms/step - loss: 0.2941 - val_loss: 0.2941\n",
            "Epoch 54/100\n",
            "367/367 [==============================] - 1s 3ms/step - loss: 0.2941 - val_loss: 0.2941\n",
            "Epoch 55/100\n",
            "367/367 [==============================] - 1s 4ms/step - loss: 0.2941 - val_loss: 0.2941\n",
            "Epoch 56/100\n",
            "367/367 [==============================] - 1s 3ms/step - loss: 0.2941 - val_loss: 0.2941\n",
            "Epoch 57/100\n",
            "367/367 [==============================] - 1s 4ms/step - loss: 0.2941 - val_loss: 0.2941\n",
            "Epoch 58/100\n",
            "367/367 [==============================] - 1s 3ms/step - loss: 0.2941 - val_loss: 0.2941\n",
            "Epoch 59/100\n",
            "367/367 [==============================] - 1s 3ms/step - loss: 0.2941 - val_loss: 0.2941\n",
            "Epoch 60/100\n",
            "367/367 [==============================] - 1s 4ms/step - loss: 0.2941 - val_loss: 0.2941\n",
            "Epoch 61/100\n",
            "367/367 [==============================] - 1s 4ms/step - loss: 0.2941 - val_loss: 0.2941\n",
            "Epoch 62/100\n",
            "367/367 [==============================] - 1s 4ms/step - loss: 0.2941 - val_loss: 0.2941\n",
            "Epoch 63/100\n",
            "367/367 [==============================] - 1s 4ms/step - loss: 0.2941 - val_loss: 0.2941\n",
            "Epoch 64/100\n",
            "367/367 [==============================] - 1s 4ms/step - loss: 0.2941 - val_loss: 0.2941\n",
            "Epoch 65/100\n",
            "367/367 [==============================] - 1s 4ms/step - loss: 0.2941 - val_loss: 0.2941\n",
            "Epoch 66/100\n",
            "367/367 [==============================] - 1s 3ms/step - loss: 0.2941 - val_loss: 0.2941\n",
            "Epoch 67/100\n",
            "367/367 [==============================] - 1s 4ms/step - loss: 0.2941 - val_loss: 0.2941\n",
            "Epoch 68/100\n",
            "367/367 [==============================] - 1s 4ms/step - loss: 0.2941 - val_loss: 0.2941\n",
            "Epoch 69/100\n",
            "367/367 [==============================] - 1s 4ms/step - loss: 0.2941 - val_loss: 0.2941\n",
            "Epoch 70/100\n",
            "367/367 [==============================] - 1s 4ms/step - loss: 0.2941 - val_loss: 0.2941\n",
            "Epoch 71/100\n",
            "367/367 [==============================] - 1s 3ms/step - loss: 0.2941 - val_loss: 0.2941\n",
            "Epoch 72/100\n",
            "367/367 [==============================] - 1s 3ms/step - loss: 0.2941 - val_loss: 0.2941\n",
            "Epoch 73/100\n",
            "367/367 [==============================] - 1s 3ms/step - loss: 0.2941 - val_loss: 0.2941\n",
            "Epoch 74/100\n",
            "367/367 [==============================] - 1s 3ms/step - loss: 0.2941 - val_loss: 0.2941\n",
            "Epoch 75/100\n",
            "367/367 [==============================] - 1s 4ms/step - loss: 0.2941 - val_loss: 0.2941\n",
            "Epoch 76/100\n",
            "367/367 [==============================] - 1s 4ms/step - loss: 0.2941 - val_loss: 0.2941\n",
            "Epoch 77/100\n",
            "367/367 [==============================] - 1s 3ms/step - loss: 0.2941 - val_loss: 0.2941\n",
            "Epoch 78/100\n",
            "367/367 [==============================] - 1s 3ms/step - loss: 0.2941 - val_loss: 0.2941\n",
            "Epoch 79/100\n",
            "367/367 [==============================] - 1s 3ms/step - loss: 0.2941 - val_loss: 0.2941\n",
            "Epoch 80/100\n",
            "367/367 [==============================] - 1s 4ms/step - loss: 0.2941 - val_loss: 0.2941\n",
            "Epoch 81/100\n",
            "367/367 [==============================] - 1s 4ms/step - loss: 0.2941 - val_loss: 0.2941\n",
            "Epoch 82/100\n",
            "367/367 [==============================] - 1s 3ms/step - loss: 0.2941 - val_loss: 0.2941\n",
            "Epoch 83/100\n",
            "367/367 [==============================] - 1s 4ms/step - loss: 0.2941 - val_loss: 0.2941\n",
            "Epoch 84/100\n",
            "367/367 [==============================] - 1s 3ms/step - loss: 0.2941 - val_loss: 0.2941\n",
            "Epoch 85/100\n",
            "367/367 [==============================] - 1s 4ms/step - loss: 0.2941 - val_loss: 0.2941\n",
            "Epoch 86/100\n",
            "367/367 [==============================] - 1s 4ms/step - loss: 0.2941 - val_loss: 0.2941\n",
            "Epoch 87/100\n",
            "367/367 [==============================] - 1s 4ms/step - loss: 0.2941 - val_loss: 0.2941\n",
            "Epoch 88/100\n",
            "367/367 [==============================] - 1s 3ms/step - loss: 0.2941 - val_loss: 0.2941\n",
            "Epoch 89/100\n",
            "367/367 [==============================] - 1s 3ms/step - loss: 0.2941 - val_loss: 0.2941\n",
            "Epoch 90/100\n",
            "367/367 [==============================] - 1s 4ms/step - loss: 0.2941 - val_loss: 0.2941\n",
            "Epoch 91/100\n",
            "367/367 [==============================] - 1s 4ms/step - loss: 0.2941 - val_loss: 0.2941\n",
            "Epoch 92/100\n",
            "367/367 [==============================] - 1s 3ms/step - loss: 0.2941 - val_loss: 0.2941\n",
            "Epoch 93/100\n",
            "367/367 [==============================] - 1s 4ms/step - loss: 0.2941 - val_loss: 0.2941\n",
            "Epoch 94/100\n",
            "367/367 [==============================] - 1s 4ms/step - loss: 0.2941 - val_loss: 0.2941\n",
            "Epoch 95/100\n",
            "367/367 [==============================] - 1s 4ms/step - loss: 0.2941 - val_loss: 0.2941\n",
            "Epoch 96/100\n",
            "367/367 [==============================] - 1s 4ms/step - loss: 0.2941 - val_loss: 0.2941\n",
            "Epoch 97/100\n",
            "367/367 [==============================] - 1s 4ms/step - loss: 0.2941 - val_loss: 0.2941\n",
            "Epoch 98/100\n",
            "367/367 [==============================] - 1s 4ms/step - loss: 0.2941 - val_loss: 0.2941\n",
            "Epoch 99/100\n",
            "367/367 [==============================] - 1s 4ms/step - loss: 0.2941 - val_loss: 0.2941\n",
            "Epoch 100/100\n",
            "367/367 [==============================] - 1s 4ms/step - loss: 0.2941 - val_loss: 0.2941\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEICAYAAAC0+DhzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3de3RV5bnv8e9jEi4hoHLxBii4QSxKSCARW6qCpWODeMAqVFGLHOt11Gq1bkprFY7djHFUxq6HXe2Q1lv3scVby8Etbt1SKFi0FYSioLaRogSpIsrNECDxOX/MObNm5lohKyEhkPX7jDEHc83rO7M0v/U+850r5u6IiIjEHdXWDRARkcOPwkFERNIoHEREJI3CQURE0igcREQkjcJBRETSKBxEMjCzF8zsqpbetoltGGVmlS19XJFs5Ld1A0Raipntjr0sBPYCteHr6939iWyP5e7jWmNbkSOFwkHaDXcviubNbCNwjbu/nNzOzPLdveZQtk3kSKOykrR7UXnGzH5gZv8AHjWzY83sP81sq5l9Fs73ie2z1MyuCeenmdkrZjYn3PbvZjaumdv2N7NlZrbLzF42swfM7P9meR1fCs+13czWmdmE2LoLzGx9eNzNZnZ7uLxneG3bzexTM1tuZvr/Xhql/0gkV5wAdAdOAa4j+G//0fD1ycAe4GcH2H8E8C7QE7gXeNjMrBnb/hr4M9ADmAV8K5vGm1kB8BzwEnAc8F3gCTMbFG7yMEHprCtwJvD7cPn3gUqgF3A88CNA35kjjVI4SK74Apjp7nvdfY+7b3P3Z929yt13AbOB8w6w//vu/gt3rwUeB04k+GWb9bZmdjJQDtzl7vvc/RVgYZbtPxsoAv53uO/vgf8EpoTr9wODzaybu3/m7m/Elp8InOLu+919uesL1SQLCgfJFVvdvTp6YWaFZvaQmb1vZjuBZcAxZpbXwP7/iGbcvSqcLWriticBn8aWAWzKsv0nAZvc/YvYsveB3uH8JcAFwPtm9gcz+3K4/D6gAnjJzDaY2Ywszyc5TuEguSL5afn7wCBghLt3A84NlzdUKmoJW4DuZlYYW9Y3y30/BPom7hecDGwGcPfX3X0iQclpAfBUuHyXu3/f3U8FJgC3mdnXDvI6JAcoHCRXdSW4z7DdzLoDM1v7hO7+PrASmGVmHcJP9/8jy93/BFQB082swMxGhfvOD491hZkd7e77gZ0EZTTM7EIzGxDe89hBMLT3i8ynEElROEiuuh/oDHwCvAb81yE67xXAl4FtwL8CTxI8j3FA7r6PIAzGEbT5QWCqu78TbvItYGNYIrshPA/AQOBlYDfwKvCguy9psauRdst0b0qk7ZjZk8A77t7qPReRplDPQeQQMrNyM/snMzvKzMYCEwnuEYgcVvSEtMihdQLwW4LnHCqBG919dds2SSSdykoiIpJGZSUREUnTLspKPXv29H79+rV1M0REjiirVq36xN17ZVrXLsKhX79+rFy5sq2bISJyRDGz9xtap7KSiIikUTiIiEgahYOIiKRpF/ccROTQ279/P5WVlVRXVze+sbSpTp060adPHwoKCrLeR+EgIs1SWVlJ165d6devHw3/3SNpa+7Otm3bqKyspH///lnvp7KSiDRLdXU1PXr0UDAc5syMHj16NLmHp3AQkWZTMBwZmvM+5XQ4vPIK3Hkn7N/f1i0RETm85HQ4vPYa/Ou/wt5Gv01fRA4327Zto6SkhJKSEk444QR69+5d93rfvn0H3HflypXcfPPNjZ7jK1/5Sou0denSpVx44YUtcqxDJadvSOeHV19T07btEJGm69GjB2vWrAFg1qxZFBUVcfvtt9etr6mpIT8/86+4srIyysrKGj3HihUrWqaxR6Cc7jlE/92orCTSPkybNo0bbriBESNGMH36dP785z/z5S9/mdLSUr7yla/w7rvvAvU/yc+aNYurr76aUaNGceqppzJ37ty64xUVFdVtP2rUKCZNmsTpp5/OFVdcQfSN1osWLeL0009n+PDh3HzzzY32ED799FMuuugiiouLOfvss1m7di0Af/jDH+p6PqWlpezatYstW7Zw7rnnUlJSwplnnsny5ctb/GfWkJzuOURDftVzEDk43/sehB/iW0xJCdx/f9P3q6ysZMWKFeTl5bFz506WL19Ofn4+L7/8Mj/60Y949tln0/Z55513WLJkCbt27WLQoEHceOONac8ErF69mnXr1nHSSScxcuRI/vjHP1JWVsb111/PsmXL6N+/P1OmTGm0fTNnzqS0tJQFCxbw+9//nqlTp7JmzRrmzJnDAw88wMiRI9m9ezedOnVi3rx5/PM//zN33HEHtbW1VFVVNf0H0kw5HQ4qK4m0P5MnTyYvLw+AHTt2cNVVV/G3v/0NM2N/A2WC8ePH07FjRzp27Mhxxx3HRx99RJ8+feptc9ZZZ9UtKykpYePGjRQVFXHqqafWPT8wZcoU5s2bd8D2vfLKK3UBdf7557Nt2zZ27tzJyJEjue2227jiiiu4+OKL6dOnD+Xl5Vx99dXs37+fiy66iJKSkoP62TSFwgGVlUQOVnM+4beWLl261M3feeedjB49mt/97nds3LiRUaNGZdynY8eOdfN5eXnUZPjEmM02B2PGjBmMHz+eRYsWMXLkSF588UXOPfdcli1bxvPPP8+0adO47bbbmDp1aouetyE5fc9BZSWR9m3Hjh307t0bgMcee6zFjz9o0CA2bNjAxo0bAXjyyScb3eecc87hiSeeAIJ7GT179qRbt2689957DBkyhB/84AeUl5fzzjvv8P7773P88cdz7bXXcs011/DGG2+0+DU0JKfDQT0HkfZt+vTp/PCHP6S0tLTFP+kDdO7cmQcffJCxY8cyfPhwunbtytFHH33AfWbNmsWqVasoLi5mxowZPP744wDcf//9nHnmmRQXF1NQUMC4ceNYunQpQ4cOpbS0lCeffJJbbrmlxa+hIe3ib0iXlZV5c/7Yz29/C5dcEtxIGzq0FRom0o69/fbbfOlLX2rrZrS53bt3U1RUhLvzne98h4EDB3Lrrbe2dbPSZHq/zGyVu2cc05vTPQeVlUTkYP3iF7+gpKSEM844gx07dnD99de3dZNahG5Io7KSiDTfrbfeelj2FA5WTvccNJRVRCSznA4HlZVERDLLKhzMbKyZvWtmFWY2I8P628xsvZmtNbPFZnZKbN09ZvZWOF2aYd+5ZrY79nqamW01szXhdE1zL64xKiuJiGTW6D0HM8sDHgC+DlQCr5vZQndfH9tsNVDm7lVmdiNwL3CpmY0HhgElQEdgqZm94O47w2OXAcdmOO2T7n7TwVxYNtRzEBHJLJuew1lAhbtvcPd9wHxgYnwDd1/i7tGXfrwGRM+dDwaWuXuNu38OrAXGQl3o3AdMP/jLaB7dcxA5co0ePZoXX3yx3rL777+fG2+8scF9Ro0aRTTs/YILLmD79u1p28yaNYs5c+Yc8NwLFixg/frU5+O77rqLl19+uSnNz+hw+mrvbMKhN7Ap9royXNaQbwMvhPN/AcaaWaGZ9QRGA33DdTcBC919S4ZjXBKWqJ4xs74Z1mNm15nZSjNbuXXr1iwuI53KSiJHrilTpjB//vx6y+bPn5/Vl99B8G2qxxxzTLPOnQyHu+++mzFjxjTrWIerFr0hbWZXAmUEPQLc/SVgEbAC+A3wKlBrZicBk4F/z3CY54B+7l4M/DfweKZzufs8dy9z97JevXo1q70qK4kcuSZNmsTzzz9f94d9Nm7cyIcffsg555zDjTfeSFlZGWeccQYzZ87MuH+/fv345JNPAJg9ezannXYaX/3qV+u+1huCZxjKy8sZOnQol1xyCVVVVaxYsYKFCxfyL//yL5SUlPDee+8xbdo0nnnmGQAWL15MaWkpQ4YM4eqrr2Zv+NfE+vXrx8yZMxk2bBhDhgzhnXfeOeD1tfVXe2fznMNmUp/2ISgZbU5uZGZjgDuA89y97m+ruftsYHa4za+BvwKlwACgIvzbpoVmVuHuA9x9W+ywvyS4f9EqVFYSaSFt8J3d3bt356yzzuKFF15g4sSJzJ8/n29+85uYGbNnz6Z79+7U1tbyta99jbVr11JcXJzxOKtWrWL+/PmsWbOGmpoahg0bxvDhwwG4+OKLufbaawH48Y9/zMMPP8x3v/tdJkyYwIUXXsikSZPqHau6uppp06axePFiTjvtNKZOncrPf/5zvve97wHQs2dP3njjDR588EHmzJnDL3/5ywavr62/2jubnsPrwEAz629mHYDLgIXxDcysFHgImODuH8eW55lZj3C+GCgGXnL35939BHfv5+79gCp3HxBud2Ls0BOAt5t/eQemspLIkS1eWoqXlJ566imGDRtGaWkp69atq1cCSlq+fDnf+MY3KCwspFu3bkyYMKFu3VtvvcU555zDkCFDeOKJJ1i3bt0B2/Puu+/Sv39/TjvtNACuuuoqli1bVrf+4osvBmD48OF1X9bXkFdeeYVvfetbQOav9p47dy7bt28nPz+f8vJyHn30UWbNmsWbb75J165dD3jsbDTac3D3GjO7CXgRyAMecfd1ZnY3sNLdFxKUkYqAp8OewAfuPgEoAJaHy3YCV7p7Y5/TbzazCUAN8CkwrVlXlgWVlURaSBt9Z/fEiRO59dZbeeONN6iqqmL48OH8/e9/Z86cObz++usce+yxTJs2jerq6mYdf9q0aSxYsIChQ4fy2GOPsXTp0oNqb/S13wfzld+H6qu9s7rn4O6L3P00d/+nsEyEu98VBgPuPsbdj3f3knCaEC6vdvfB4XS2u2fsd7p7UWz+h+5+hrsPdffR7n7gwtxBUFlJ5MhWVFTE6NGjufrqq+t6DTt37qRLly4cffTRfPTRR7zwwgsHPMa5557LggUL2LNnD7t27eK5556rW7dr1y5OPPFE9u/fX/c12wBdu3Zl165daccaNGgQGzdupKKiAoD/+I//4LzzzmvWtbX1V3vru5VQWUnkSDZlyhS+8Y1v1JWXoq+4Pv300+nbty8jR4484P7Dhg3j0ksvZejQoRx33HGUl5fXrfvJT37CiBEj6NWrFyNGjKgLhMsuu4xrr72WuXPn1t2IBujUqROPPvookydPpqamhvLycm644YZmXVf0t62Li4spLCys99XeS5Ys4aijjuKMM85g3LhxzJ8/n/vuu4+CggKKior41a9+1axzxuX0V3Zv3w7HHgs//WlwP01Esqev7D6y6Cu7m0BlJRGRzBQOqKwkIpKU0+Gg0UoiB6c9lKVzQXPep5wOh6PCq1c4iDRdp06d2LZtmwLiMOfubNu2jU6dOjVpv5werWQWlJZUVhJpuj59+lBZWUlzv9tMDp1OnTrRp0+fxjeMyelwgKC0pJ6DSNMVFBTQv3//tm6GtJKcLitB0HNQOIiI1KdwUFlJRCRNzoeDykoiIulyPhxUVhIRSadwUFlJRCRNzoeDykoiIulyPhzUcxARSadw0D0HEZE0OR8OKiuJiKTL+XBQWUlEJF3Oh4N6DiIi6XI+HHTPQUQkncJBZSURkTQ5Hw4qK4mIpMv5cFBZSUQkncJBZSURkTQ5Hw4qK4mIpMv5cFBZSUQkncJBZSURkTQ5Hw4qK4mIpMv5cFBZSUQkncJBZSURkTQ5Hw4qK4mIpMsqHMxsrJm9a2YVZjYjw/rbzGy9ma01s8Vmdkps3T1m9lY4XZph37lmtjvD8kvMzM2srKkX1RQqK4mIpGs0HMwsD3gAGAcMBqaY2eDEZquBMncvBp4B7g33HQ8MA0qAEcDtZtYtduwy4NgM5+wK3AL8qRnX1CQqK4mIpMum53AWUOHuG9x9HzAfmBjfwN2XuHtV+PI1oE84PxhY5u417v45sBYYC3Whcx8wPcM5fwLcA1Q38XqaTGUlEZF02YRDb2BT7HVluKwh3wZeCOf/Aow1s0Iz6wmMBvqG624CFrr7lvjOZjYM6Ovuz2fRtoMWlZXcD8XZRESODPkteTAzuxIoA84DcPeXzKwcWAFsBV4Fas3sJGAyMCqx/1HAvwHTsjjXdcB1ACeffHKz25wf/gRqa1PzIiK5Lpuew2ZSn/YhKBltTm5kZmOAO4AJ7r43Wu7us929xN2/DhjwV6AUGABUmNlGoNDMKoCuwJnA0nD52cDCTDel3X2eu5e5e1mvXr2yuthMCgqCf1VaEhFJyeaz8uvAQDPrTxAKlwGXxzcws1LgIWCsu38cW54HHOPu28ysGCgGXnL3GuCE2Ha73X1A+LJnbPlS4HZ3X9mci8tG1FtQOIiIpDQaDu5eY2Y3AS8CecAj7r7OzO4GVrr7QoIby0XA02YG8IG7TwAKgOXhsp3AlWEwHDaicNCIJRGRlKyq7O6+CFiUWHZXbH5MA/tVE4xYauz4RQ0sH5VN+w6GykoiIuly/glplZVERNLlfDhEPQeVlUREUnI+HNRzEBFJp3DQDWkRkTQ5Hw66IS0iki7nw0FlJRGRdAoHlZVERNLkfDiorCQiki7nw0FlJRGRdAoHlZVERNLkfDiorCQiki7nw0FlJRGRdAoHlZVERNLkfDiorCQiki7nw0FlJRGRdAoHlZVERNLkfDiorCQiki7nw0FlJRGRdAoHlZVERNLkfDiorCQiki7nw0FlJRGRdAoHlZVERNLkfDiorCQiki7nw0FlJRGRdDkfDkeFPwGVlUREUnI+HMyC0pJ6DiIiKTkfDhCUlhQOIiIpCgeCnoPKSiIiKQoH1HMQEUlSOKBwEBFJUjigspKISJLCAfUcRESSsgoHMxtrZu+aWYWZzciw/jYzW29ma81ssZmdElt3j5m9FU6XZth3rpntjr2+wczeNLM1ZvaKmQ1u7sVlKz9fPQcRkbhGw8HM8oAHgHHAYGBKhl/Yq4Eydy8GngHuDfcdDwwDSoARwO1m1i127DLg2MSxfu3uQ9y9JDzOvzXnwppCzzmIiNSXTc/hLKDC3Te4+z5gPjAxvoG7L3H3qvDla0CfcH4wsMzda9z9c2AtMBbqQuc+YHriWDtjL7sA3rRLajqVlURE6ssmHHoDm2KvK8NlDfk28EI4/xdgrJkVmllPYDTQN1x3E7DQ3bckD2Bm3zGz9wh6DjdnOomZXWdmK81s5datW7O4jIaprCQiUl+L3pA2syuBMoIeAe7+ErAIWAH8BngVqDWzk4DJwL9nOo67P+Du/wT8APhxA9vMc/cydy/r1avXQbVbZSURkfqyCYfNpD7tQ1Ay2pzcyMzGAHcAE9x9b7Tc3We7e4m7fx0w4K9AKTAAqDCzjUChmVVkOPd84KIsr6XZVFYSEakvP4ttXgcGmll/glC4DLg8voGZlQIPAWPd/ePY8jzgGHffZmbFQDHwkrvXACfEttvt7gPC+YHu/rdw1Xggmm81KiuJiNTXaDi4e42Z3QS8COQBj7j7OjO7G1jp7gsJykhFwNNmBvCBu08ACoDl4bKdwJVhMBzITWEvZD/wGXBV8y4tewUFsG9fa59FROTIkU3PAXdfRHDvIL7srtj8mAb2qyYYsdTY8Yti87dk06aWlJ8PVVWNbycikiv0hDQqK4mIJCkc0GglEZEkhQMarSQikqRwQGUlEZEkhQMqK4mIJCkcUFlJRCRJ4YDKSiIiSQoHVFYSEUlSOKCykohIksIB/Q1pEZEkhQPqOYiIJCkcUDiIiCQpHEjdkPZW/4OkIiJHBoUDQc8BoLa2bdshInK4UDiQCgeVlkREAgoHgrISaMSSiEhE4YB6DiIiSQoHFA4iIkkKB1RWEhFJUjignoOISJLCgVQ4qOcgIhJQOJAqK6nnICISUDigspKISJLCAZWVRESSFA6orCQikqRwQGUlEZEkhQMqK4mIJCkcUFlJRCRJ4YDKSiIiSQoHVFYSEUlSOKCykohIksIBlZVERJKyCgczG2tm75pZhZnNyLD+NjNbb2ZrzWyxmZ0SW3ePmb0VTpdm2Heume3O5litRd/KKiJSX6PhYGZ5wAPAOGAwMMXMBic2Ww2UuXsx8Axwb7jveGAYUAKMAG43s26xY5cBx2ZzrNaknoOISH3Z9BzOAircfYO77wPmAxPjG7j7EnevCl++BvQJ5wcDy9y9xt0/B9YCY6EudO4Dpmd5rFajcBARqS+bcOgNbIq9rgyXNeTbwAvh/F+AsWZWaGY9gdFA33DdTcBCd9+S5bHqMbPrzGylma3cunVrFpfRMJWVRETqy2/Jg5nZlUAZcB6Au79kZuXACmAr8CpQa2YnAZOBUdkeK8nd5wHzAMrKyvxg2q2eg4hIfdn0HDaT+rQPQZlnc3IjMxsD3AFMcPe90XJ3n+3uJe7+dcCAvwKlwACgwsw2AoVmVtHYsVqLwkFEpL5seg6vAwPNrD9BKFwGXB7fwMxKgYeAse7+cWx5HnCMu28zs2KgGHjJ3WuAE2Lb7Xb3AQc6VmtSWUlEpL5Gw8Hda8zsJuBFIA94xN3XmdndwEp3X0hwY7kIeNrMAD5w9wlAAbA8XLYTuDIMhgNp6FitRj0HEZH6srrn4O6LgEWJZXfF5sc0sF81wYilxo5f1NixWpPCQUSkPj0hjcpKIiJJCgfgqKPATD0HEZGIwiGUn69wEBGJKBxCBQUqK4mIRBQOIfUcRERSFA6h/Hz1HEREIgqHUEGBeg4iIhGFQ0hlJRGRFIVDSGUlEZEUhUNIZSURkRSFQ0hlJRGRFIVDSGUlEZEUhUNIZSURkRSFQ0hlJRGRFIVDSGUlEZEUhUNIZSURkRSFQ0hlJRGRFIVDSN/KKiKSonAIqecgIpKicAgpHEREUhQOIZWVRERSFA4h9RxERFIUDiGFg4hIisIhpLKSiEiKwiGknoOISIrCIaRwEBFJUTiEVFYSEUlROITUcxARSVE4hBQOIiIpCoeQykoiIikKh1B+PtTWgntbt0REpO1lFQ5mNtbM3jWzCjObkWH9bWa23szWmtliMzsltu4eM3srnC7NsO9cM9sde32umb1hZjVmNqm5F9ZU+fnBv7W1h+qMIiKHr0bDwczygAeAccBgYIqZDU5sthooc/di4Bng3nDf8cAwoAQYAdxuZt1ixy4Djk0c6wNgGvDrZlxPsxUUBP+qtCQikl3P4Sygwt03uPs+YD4wMb6Buy9x96rw5WtAn3B+MLDM3Wvc/XNgLTAW6kLnPmB64lgb3X0t8EUzr6lZop6DbkqLiGQXDr2BTbHXleGyhnwbeCGc/wsw1swKzawnMBroG667CVjo7lua1uSAmV1nZivNbOXWrVubc4h6onBQz0FEBPJb8mBmdiVQBpwH4O4vmVk5sALYCrwK1JrZScBkYFRzz+Xu84B5AGVlZQd9GzkqK6nnICKSXc9hM6lP+xCUjDYnNzKzMcAdwAR33xstd/fZ7l7i7l8HDPgrUAoMACrMbCNQaGYVzb6KFhD1HPbsactWiIgcHrIJh9eBgWbW38w6AJcBC+MbmFkp8BBBMHwcW55nZj3C+WKgGHjJ3Z939xPcvZ+79wOq3H1Ay1xS85xxBpjBBRfA+vVt2RIRkbbXaDi4ew3B/YEXgbeBp9x9nZndbWYTws3uA4qAp81sjZlF4VEALDez9QQloCvD4zXIzMrNrJKg7PSQma1r1pU10ciR8OKLsHUrlJXBww9rWKuI5C7zdvDUV1lZma9cubLpO37xRZAA0Q0HYMsWuPxyWLoUjj8eJk2CyZOhvBwKC1uuzSIibc3MVrl7WaZ1uf2E9Pz5UFwMzz1X92j0iSfCyy/D00/DOefAI4/AqFHQtSucfnoQFHfcEfQsliyBDRtg794Dn0ZE5EjToqOVjji9egWhMGFCkAD33gvl5eTlBT2GSZNg9+4gLNasgbVrYfVq+N3v0ktOxx8fTL16Qc+ecMwxcPTRwb/xqWvXoAeSnDp3Du55iIgcDnK7rATBgw2/+AXMnAmffBLcmb7kkqCLcOaZGXepqYFNm4JewwcfBPObNsHHHweH2LoVtm+HHTtg377sm1JYCF26BFMUGtF8ly5QVJQKnG7d6m/XrVtqii/v1AmOyu3+oYg04EBlJYVDZMcO+NWv4NlnYdmyoEcxfjz85CdQWtrsw1ZXB4fevh0++yzoiVRVBdPnn9efj0979qQv3707ONauXU1rQ+fOmXsq0fJoPpo6dgxCJb5fp07BrZkOHYL18eX5+cFUUBCsi6ZomXpEIocnhUNT/eMfwc2GOXOC3+iXXAJ33glDh7bcOQ5CbW16yOzcmZoaCp944OzZk3nauzf4tyWfFM/PD0IlmuIBUlCQCp14GEXro/A56qhgys9PLe/YMXW8goLUNnl5qeNmmjp0CI6Tl5faJ9ovmuLni6b4cpH2QOHQXDt2wE9/Gkw7d8KFFwZ3o88+u+XPdZiprQ16PVG4VFcHgbFvXypAqqqCf2tqgu2jddG0f3+wbt+++vsmt4nWRef7/PNgXXV18G9tbWpgWXSuthaFRTJM4r2kKFzy8lK9qGxDKb4uGa6Z9o23J1qePFayfWaZA7Wha4u3J7k+/jq+3j01xT8YJAPWrH57orbGlyXbe6CfvWRH4XCwPvsMfvYzuP9++PTTYITT5ZfDlClw8smtd17JqLY2c8i4ByFSU1M/dOKvo2Xx7ePhE83HX0f7J7eLgiq+fVzUlmiK9qmpST9HNGVqUxSw0RTtm6ktmY4TzeeCTIF4oKBs6tTQMRvaLh5kmT48NNS2ePglj5c8zuWXw3nnNe/npXBoKbt3w+OPwxNPwKuvBsuKi+H882H06OBhiBNO0EcYOSwlAyIeILW1qfA8UNBkCsZkwMXXR7/YoH7PMf5rJ967SIZa9Dr6N1OoZgrCZGhn2iY6blOnhs7b0HXEP3Qkf/YNtTv5QSX+M0h+CLnnHpg6tXn/TSgcWsOGDfDUU8E41z/+MaiBQDCOdcgQGDgw6FWcfDL06RM8QHHiicFwoiM5PL74ov7H2OQ3FTb20SyqtUQ3CUSkzSgcWtvevfDnPwcPQbz5ZjBt2BCMaU3Ky0uNTY3utsbvlEZF3+jjB9Tva0aF52SBOP4LONPHjuQUr7NEtZZoSq5L1mhaSvyObzTUqVOn9KmwMHhApKgoNdY3mo+P+S0qCrbr2rX+mODOnYOfkYjUc6BwyO2H4FpKx47B49TnnCS9pOgAAAZtSURBVFN/+Z49wYMQH34YfC/Hhx8GY1qjsanV1akidPyubU1N6hc9pPqTe/cGpa2ofx4vYsf7s5nuTJrVn48P3yksDB6eiAdUpikaHhTdWezQIRVMUTujfndDffbkTYF4PSO6Kx0Nm4rmt22D998PxvBGP7umPEACQQBFw6Gi4MgUMsmhUtHDI0cfHbyOrj8a6xufCguDdUdyz1AkpHBoTZ07w6BBwSQta//+1JCpqqogOOJTFCJVVfWDJvlAye7dwdOL0XGi7ffubd5dXLPMvZ+CglRod+iQCqlkwHTunBqnm3xoJOphxYf9JMM6muJ3NOO9TJXyJEsKBzkyRb/wunVrfNvm2r8/GMK8Y0cw7d2bKr1FYRON542m6urMvZ+olxQNtfr886DsGN8/GjPcmqXe+D2f5EMcydfJZdGY2vgDKpnGviZfJ0MqHpQNjZfN1J6oRxYf2xsfGxzvbUcaGv8aHwoUTfHzNjTmt6Ep07mPcAoHkYYUFECPHsF0qLgHZbfoIY9oSpbfouVRWEXjeeMDBeJlx0xlvOQ43WifeJDFj7N/f3CeXbvq35tKDm/KNLY2Om57FoXIgcbBJse1xqdMkg94ZBrzetddcNllLX45CgeRw0n8flDXrm3dmpYV3Y+Kh09yzGly/GcUQNE9LEgFVRRQ8YEWyfMlx4xmerAkOUY1OeiiobGyB5pvbGxr8p5cQz+v5Ljb+Dmi43Tv3vLvFQoHETlUzFJlIjns6e6UiIikUTiIiEgahYOIiKRROIiISBqFg4iIpFE4iIhIGoWDiIikUTiIiEiadvGV3Wa2FXi/mbv3BD5pweYcKXLxunPxmiE3rzsXrxmaft2nuHuvTCvaRTgcDDNb2dD3mbdnuXjduXjNkJvXnYvXDC173SoriYhIGoWDiIikUTjAvLZuQBvJxevOxWuG3LzuXLxmaMHrzvl7DiIikk49BxERSaNwEBGRNDkdDmY21szeNbMKM5vR1u1pDWbW18yWmNl6M1tnZreEy7ub2X+b2d/Cf49t67a2NDPLM7PVZvaf4ev+Zvan8P1+0sw6tHUbW5qZHWNmz5jZO2b2tpl9OUfe61vD/77fMrPfmFmn9vZ+m9kjZvaxmb0VW5bxvbXA3PDa15rZsKaeL2fDwczygAeAccBgYIqZDW7bVrWKGuD77j4YOBv4TnidM4DF7j4QWBy+bm9uAd6Ovb4H+Km7DwA+A77dJq1qXf8H+C93Px0YSnD97fq9NrPewM1AmbufCeQBl9H+3u/HgLGJZQ29t+OAgeF0HfDzpp4sZ8MBOAuocPcN7r4PmA9MbOM2tTh33+Lub4Tzuwh+WfQmuNbHw80eBy5qmxa2DjPrA4wHfhm+NuB84Jlwk/Z4zUcD5wIPA7j7PnffTjt/r0P5QGczywcKgS20s/fb3ZcBnyYWN/TeTgR+5YHXgGPM7MSmnC+Xw6E3sCn2ujJc1m6ZWT+gFPgTcLy7bwlX/QM4vo2a1VruB6YD0V+d7wFsd/for8e3x/e7P7AVeDQsp/3SzLrQzt9rd98MzAE+IAiFHcAq2v/7DQ2/twf9+y2XwyGnmFkR8CzwPXffGV/nwXjmdjOm2cwuBD5291Vt3ZZDLB8YBvzc3UuBz0mUkNrbew0Q1tknEoTjSUAX0ssv7V5Lv7e5HA6bgb6x133CZe2OmRUQBMMT7v7bcPFHUTcz/PfjtmpfKxgJTDCzjQTlwvMJavHHhGUHaJ/vdyVQ6e5/Cl8/QxAW7fm9BhgD/N3dt7r7fuC3BP8NtPf3Gxp+bw/691suh8PrwMBwREMHghtYC9u4TS0urLU/DLzt7v8WW7UQuCqcvwr4f4e6ba3F3X/o7n3cvR/B+/p7d78CWAJMCjdrV9cM4O7/ADaZ2aBw0deA9bTj9zr0AXC2mRWG/71H192u3+9QQ+/tQmBqOGrpbGBHrPyUlZx+QtrMLiCoTecBj7j77DZuUoszs68Cy4E3SdXff0Rw3+Ep4GSCrzv/prsnb3Yd8cxsFHC7u19oZqcS9CS6A6uBK919b1u2r6WZWQnBTfgOwAbgfxJ8CGzX77WZ/S/gUoLReauBawhq7O3m/Taz3wCjCL6W+yNgJrCADO9tGJI/IyivVQH/091XNul8uRwOIiKSWS6XlUREpAEKBxERSaNwEBGRNAoHERFJo3AQEZE0CgcREUmjcBARkTT/H9zvq3kJXutIAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "2.060459503028294\n",
            "0.7927917438120422\n",
            "0.3174977705307476\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0SJoNZ_atLhk",
        "outputId": "82511f91-15ce-41d9-db4c-2044054bbb34"
      },
      "source": [
        "predicted = model.predict(r_test)\n",
        "TEST = r_test\n",
        "\n",
        "Langle = []\n",
        "for i in range(0,int(tot*0.33)):\n",
        "    y_pred = np.reshape(predicted[i], [3, 3])\n",
        "\n",
        "    a1 = y_pred[:,0]\n",
        "    a2 = y_pred[:,1]\n",
        "\n",
        "    b1 = a1 / np.linalg.norm(a1)\n",
        "    b2 = a2 - np.dot(a2,b1)*b1\n",
        "    b2 = b2 / np.linalg.norm(b2)\n",
        "\n",
        "    b3 = np.cross(b1, b2)\n",
        "\n",
        "    #y_pred = np.transpose([b1, b2, b3])\n",
        "    y_pred_n = np.reshape([b1, b2, b3], [3,3])\n",
        "    y_real = np.reshape(TEST[i], [3, 3])\n",
        "\n",
        "    \n",
        "    M = np.matmul(y_real, np.linalg.inv(y_pred_n))\n",
        "    cosine = (M[0,0] + M[1,1] + M[2,2] - 1)/2\n",
        "\n",
        "    #M = np.dot(y_real,  1/y_pred)\n",
        "    #cosine = (M - 1)/2\n",
        "    if cosine > 1:\n",
        "        cosine = 1\n",
        "    \n",
        "    if cosine < -1:\n",
        "        cosine = -1\n",
        "    \n",
        "    if i < 4:\n",
        "        #print(predicted[i])\n",
        "        #print(y_pred)\n",
        "        #print('---')\n",
        "        #print(y_pred_n)\n",
        "        #print(y_pred_n)\n",
        "        print('---')\n",
        "        #print(y_real)\n",
        "        print(M)\n",
        "        print('-----------')\n",
        "    \n",
        "    #print(acos(cosine))\n",
        "    Langle = np.append(Langle, acos(cosine))\n",
        "\n",
        "\n",
        "\n",
        "print(np.max(Langle)*180/pi)\n",
        "print(np.average(Langle)*180/pi)\n",
        "print(np.std(Langle)*180/pi)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---\n",
            "[[ 0.99983226  0.01443583  0.01127558]\n",
            " [-0.0144542   0.99989435  0.00155153]\n",
            " [-0.01125197 -0.00171427  0.99993531]]\n",
            "-----------\n",
            "---\n",
            "[[ 0.99987199  0.01565603  0.00330194]\n",
            " [-0.01572561  0.99962996  0.02219729]\n",
            " [-0.00295317 -0.02224639  0.99974822]]\n",
            "-----------\n",
            "---\n",
            "[[ 0.99990812 -0.00770133  0.01115709]\n",
            " [ 0.00812657  0.99922228 -0.03858389]\n",
            " [-0.01085126  0.03867102  0.99919306]]\n",
            "-----------\n",
            "---\n",
            "[[ 0.99993698  0.00863511  0.00717411]\n",
            " [-0.00867239  0.99994902  0.00518099]\n",
            " [-0.007129   -0.00524287  0.99996087]]\n",
            "-----------\n",
            "2.5361841029956356\n",
            "1.0596096645778739\n",
            "0.41207780758465157\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_Ew-CNZPpuR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5hyTTJtQuNA"
      },
      "source": [
        "Q = R.random(random_state = i).as_quat()\n",
        "print(Q)\n",
        "Rot = Q[0] + Q[1]*e13 +Q[2]*e23 + Q[3]*e12\n",
        "B = -2*general_logarithm(Rot)\n",
        "\n",
        "print(B)\n",
        "print('----')\n",
        "Q = - Q\n",
        "print(Q)\n",
        "Rot = Q[0] + Q[1]*e13 +Q[2]*e23 + Q[3]*e12\n",
        "B = -2*general_logarithm(Rot)\n",
        "print(B)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzO_EQ2NQ4tj"
      },
      "source": [
        "!zip -r /content/ /content\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"/content/.zip\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}