{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " LearningRotations_HigherDimension.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNcNgUDPymQG01RkP2U8HHw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/albertomariapepe/Learning-Rotations/blob/main/LearningRotations_HigherDimension.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "525_Em9E0nxc"
      },
      "source": [
        "!pip install git+https://github.com/pygae/clifford.git@master"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHCD47lEXWMy"
      },
      "source": [
        "!pip install tensorflow_graphics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3aVTpc0S_6hG",
        "outputId": "a9d56e6c-9089-432a-c73e-1ed12e4d3cf7"
      },
      "source": [
        "from scipy.spatial.transform import Rotation as R\n",
        "import numpy as np\n",
        "from clifford.g3c import *\n",
        "from clifford.tools.g3c import *\n",
        "from clifford.tools.g3c.rotor_parameterisation import *\n",
        "from tensorflow.keras.layers import Input, Dense, LeakyReLU\n",
        "from keras import backend as K\n",
        "from sklearn.model_selection import train_test_split\n",
        "from math import acos\n",
        "import tensorflow as tf\n",
        "from matplotlib import pyplot as plt\n",
        "import keras\n",
        "from math import e, pi\n",
        "import tensorflow_graphics as tfg\n",
        "import tensorflow_graphics.geometry.transformation as tfg_transformation\n",
        "\n",
        "#THESE TWO ARE REQUIRED TO CONVERT TENSOR TO NUMPY ARRAY\n",
        "#tf.compat.v1.enable_eager_execution()\n",
        "#tf.config.run_functions_eagerly(True)\n",
        "\n",
        "#GENERATING THE RANDOM DATASET\n",
        "tot = int(1e5)\n",
        "'''\n",
        "i = 0\n",
        "from scipy.stats import special_ortho_group\n",
        "\n",
        "x = []\n",
        "n = [] \n",
        "for i in range(0,tot):\n",
        "\n",
        "    X = special_ortho_group.rvs(5)\n",
        "\n",
        "    #print(X)\n",
        "    #print('----')\n",
        "    x = np.append(x, X)\n",
        "\n",
        "    a1 = X[:,0]\n",
        "    a2 = X[:,1]\n",
        "    a3 = X[:,2]\n",
        "    a4 = X[:,3]\n",
        "\n",
        "    b1 = a1 / np.linalg.norm(a1)\n",
        "\n",
        "    b2 = a2 - np.dot(a1,b1)*b1\n",
        "    b2 = b2 / np.linalg.norm(b2)\n",
        "\n",
        "    b3 = a3 - (np.dot(a3,b1)*b1) - np.dot(a3,b2)*b2\n",
        "    b3 = b3 / np.linalg.norm(b3)\n",
        "\n",
        "    b4 = a4 - (np.dot(a4,b1)*b1) - np.dot(a4,b2)*b2 - np.dot(a4,b3)*b3\n",
        "\n",
        "    e1 = [1, 0, 0, 0, 0]\n",
        "    e2 = [0, 1, 0, 0, 0]\n",
        "    e3 = [0, 0, 1, 0, 0]\n",
        "    e4 = [0, 0, 0, 1, 0]\n",
        "    e5 = [0, 0, 0, 0, 1]\n",
        "\n",
        "    b5 = np.zeros(5,)\n",
        "    b5[0] = np.linalg.det(np.transpose([b1, b2, b3, b4, e1]))\n",
        "    b5[1] = np.linalg.det(np.transpose([b1, b2, b3, b4, e2]))\n",
        "    b5[2] = np.linalg.det(np.transpose([b1, b2, b3, b4, e3]))\n",
        "    b5[3] = np.linalg.det(np.transpose([b1, b2, b3, b4, e4]))\n",
        "    b5[4] = np.linalg.det(np.transpose([b1, b2, b3, b4, e5]))\n",
        "\n",
        "\n",
        "    B = np.transpose([b1, b2, b3, b4, b5])\n",
        "\n",
        "    n = np.append(n, B)\n",
        "    #n = np.append(n, np.transpose([b3]))\n",
        "\n",
        "np.save('5D-rotation-matrix.npy', x)\n",
        "np.save('20D-continuous-repres.npy', n)\n",
        "'''\n",
        "x = np.load('5D-rotation-matrix.npy')\n",
        "n = np.load('20D-continuous-repres.npy')\n",
        "\n",
        "x = np.reshape(x, [tot, 25])   \n",
        "n = np.reshape(n, [tot, 25])  \n",
        "\n",
        "#Train - Test Split\n",
        "x_train, x_test = train_test_split(x, test_size=0.33, shuffle=False)\n",
        "n_train, n_test = train_test_split(n, test_size=0.33, shuffle=False)\n",
        "\n",
        "TRAIN = n_train\n",
        "TEST = n_test\n",
        "out_size = 25\n",
        "\n",
        "nb_epoch = 200\n",
        "batch_size = 64\n",
        "\n",
        "\n",
        "r_input = Input(shape=(25))\n",
        "x = Dense(128)(r_input)\n",
        "x = LeakyReLU(alpha=0.2)(x)\n",
        "x = Dense(128)(x)\n",
        "x = LeakyReLU(alpha=0.2)(x)\n",
        "x = Dense(128)(x)\n",
        "x = LeakyReLU(alpha=0.2)(x)\n",
        "out = Dense(out_size)(x)\n",
        "print(out)\n",
        "\n",
        "\n",
        "model = keras.Model(r_input,  out)\n",
        "model.summary()\n",
        "model.compile(loss='mae', optimizer=\"adam\")\n",
        "#es_callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
        "model_train = model.fit(x = x_train, y = TRAIN, \n",
        "                        validation_split=0.3,\n",
        "                        epochs=nb_epoch,\n",
        "                        verbose=1,\n",
        "                        batch_size=batch_size)\n",
        "\n",
        "\n",
        "\n",
        "loss = model_train.history['loss']\n",
        "val_loss = model_train.history['val_loss']\n",
        "epochs = range(nb_epoch)\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, 'b-', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'r-', label='Validation loss')\n",
        "plt.title('Training loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "predicted = model.predict(x_test)\n",
        "\n",
        "M = []\n",
        "Langle = []\n",
        "\n",
        "for i in range(0,int(tot*0.33)):\n",
        "    y_pred = np.reshape(predicted[i], [5, 5])\n",
        "    y_real = np.reshape(n_test[i], [5, 5])\n",
        "\n",
        "    M = np.matmul(y_real, np.linalg.inv(y_pred))\n",
        "    cosine = (M[0,0] + M[1,1] + M[2,2] + M[3,3] + M[4,4]  - 3)/2\n",
        "\n",
        "    #M = np.dot(y_real,  1/y_pred)\n",
        "    #cosine = (M - 1)/2\n",
        "    if cosine > 1:\n",
        "        cosine = 1\n",
        "    \n",
        "    if cosine < -1:\n",
        "        cosine = -1\n",
        "    \n",
        "    #print(acos(cosine))\n",
        "    Langle = np.append(Langle, acos(cosine))\n",
        "\n",
        "\n",
        "\n",
        "print(np.max(Langle)*180/pi)\n",
        "print(np.average(Langle)*180/pi)\n",
        "print(np.std(Langle)*180/pi)\n",
        "\n",
        "#np.save('B_loss.npy', loss)\n",
        "#np.save('B_val_loss.npy', val_loss)\n",
        "#np.save('B_langle.npy', Langle)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "KerasTensor(type_spec=TensorSpec(shape=(None, 25), dtype=tf.float32, name=None), name='dense_7/BiasAdd:0', description=\"created by layer 'dense_7'\")\n",
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "Total params: 39,577\n",
            "Trainable params: 39,577\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/200\n",
            "733/733 [==============================] - 3s 4ms/step - loss: 0.1085 - val_loss: 0.0308\n",
            "Epoch 2/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0281 - val_loss: 0.0228\n",
            "Epoch 3/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0223 - val_loss: 0.0192\n",
            "Epoch 4/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0184 - val_loss: 0.0167\n",
            "Epoch 5/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0149 - val_loss: 0.0136\n",
            "Epoch 6/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0130 - val_loss: 0.0138\n",
            "Epoch 7/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0119 - val_loss: 0.0118\n",
            "Epoch 8/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0112 - val_loss: 0.0111\n",
            "Epoch 9/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0107 - val_loss: 0.0106\n",
            "Epoch 10/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0105 - val_loss: 0.0098\n",
            "Epoch 11/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0100 - val_loss: 0.0095\n",
            "Epoch 12/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0101 - val_loss: 0.0102\n",
            "Epoch 13/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0100 - val_loss: 0.0098\n",
            "Epoch 14/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0101 - val_loss: 0.0105\n",
            "Epoch 15/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0099 - val_loss: 0.0098\n",
            "Epoch 16/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0096 - val_loss: 0.0097\n",
            "Epoch 17/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0095 - val_loss: 0.0099\n",
            "Epoch 18/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0099 - val_loss: 0.0096\n",
            "Epoch 19/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0097 - val_loss: 0.0090\n",
            "Epoch 20/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0095 - val_loss: 0.0121\n",
            "Epoch 21/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0095 - val_loss: 0.0091\n",
            "Epoch 22/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0093 - val_loss: 0.0092\n",
            "Epoch 23/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0094 - val_loss: 0.0106\n",
            "Epoch 24/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0092 - val_loss: 0.0089\n",
            "Epoch 25/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0092 - val_loss: 0.0084\n",
            "Epoch 26/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0092 - val_loss: 0.0087\n",
            "Epoch 27/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0089 - val_loss: 0.0078\n",
            "Epoch 28/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0088 - val_loss: 0.0074\n",
            "Epoch 29/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0086 - val_loss: 0.0086\n",
            "Epoch 30/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0089 - val_loss: 0.0094\n",
            "Epoch 31/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0088 - val_loss: 0.0074\n",
            "Epoch 32/200\n",
            "733/733 [==============================] - 3s 4ms/step - loss: 0.0086 - val_loss: 0.0078\n",
            "Epoch 33/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0085 - val_loss: 0.0104\n",
            "Epoch 34/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0089 - val_loss: 0.0081\n",
            "Epoch 35/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0085 - val_loss: 0.0090\n",
            "Epoch 36/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0086 - val_loss: 0.0085\n",
            "Epoch 37/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0084 - val_loss: 0.0076\n",
            "Epoch 38/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0085 - val_loss: 0.0101\n",
            "Epoch 39/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0086 - val_loss: 0.0084\n",
            "Epoch 40/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0082 - val_loss: 0.0074\n",
            "Epoch 41/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0082 - val_loss: 0.0100\n",
            "Epoch 42/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0082 - val_loss: 0.0073\n",
            "Epoch 43/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0082 - val_loss: 0.0089\n",
            "Epoch 44/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0079 - val_loss: 0.0077\n",
            "Epoch 45/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0083 - val_loss: 0.0086\n",
            "Epoch 46/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0080 - val_loss: 0.0090\n",
            "Epoch 47/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0080 - val_loss: 0.0078\n",
            "Epoch 48/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0082 - val_loss: 0.0096\n",
            "Epoch 49/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0080 - val_loss: 0.0073\n",
            "Epoch 50/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0080 - val_loss: 0.0074\n",
            "Epoch 51/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0078 - val_loss: 0.0069\n",
            "Epoch 52/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0077 - val_loss: 0.0077\n",
            "Epoch 53/200\n",
            "733/733 [==============================] - 3s 3ms/step - loss: 0.0077 - val_loss: 0.0071\n",
            "Epoch 54/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0076 - val_loss: 0.0088\n",
            "Epoch 55/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0075 - val_loss: 0.0071\n",
            "Epoch 56/200\n",
            "733/733 [==============================] - 3s 4ms/step - loss: 0.0077 - val_loss: 0.0085\n",
            "Epoch 57/200\n",
            "733/733 [==============================] - 3s 4ms/step - loss: 0.0076 - val_loss: 0.0075\n",
            "Epoch 58/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0075 - val_loss: 0.0069\n",
            "Epoch 59/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0075 - val_loss: 0.0072\n",
            "Epoch 60/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0074 - val_loss: 0.0088\n",
            "Epoch 61/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0076 - val_loss: 0.0079\n",
            "Epoch 62/200\n",
            "733/733 [==============================] - 3s 3ms/step - loss: 0.0076 - val_loss: 0.0074\n",
            "Epoch 63/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0076 - val_loss: 0.0072\n",
            "Epoch 64/200\n",
            "733/733 [==============================] - 3s 4ms/step - loss: 0.0074 - val_loss: 0.0075\n",
            "Epoch 65/200\n",
            "733/733 [==============================] - 3s 4ms/step - loss: 0.0074 - val_loss: 0.0072\n",
            "Epoch 66/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0074 - val_loss: 0.0066\n",
            "Epoch 67/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0073 - val_loss: 0.0069\n",
            "Epoch 68/200\n",
            "733/733 [==============================] - 3s 3ms/step - loss: 0.0072 - val_loss: 0.0069\n",
            "Epoch 69/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0073 - val_loss: 0.0085\n",
            "Epoch 70/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0074 - val_loss: 0.0059\n",
            "Epoch 71/200\n",
            "733/733 [==============================] - 3s 3ms/step - loss: 0.0073 - val_loss: 0.0072\n",
            "Epoch 72/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0072 - val_loss: 0.0082\n",
            "Epoch 73/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0072 - val_loss: 0.0085\n",
            "Epoch 74/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0073 - val_loss: 0.0063\n",
            "Epoch 75/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0069 - val_loss: 0.0073\n",
            "Epoch 76/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0070 - val_loss: 0.0067\n",
            "Epoch 77/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0071 - val_loss: 0.0077\n",
            "Epoch 78/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0072 - val_loss: 0.0069\n",
            "Epoch 79/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0071 - val_loss: 0.0071\n",
            "Epoch 80/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0070 - val_loss: 0.0083\n",
            "Epoch 81/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0071 - val_loss: 0.0073\n",
            "Epoch 82/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0070 - val_loss: 0.0072\n",
            "Epoch 83/200\n",
            "733/733 [==============================] - 3s 3ms/step - loss: 0.0069 - val_loss: 0.0080\n",
            "Epoch 84/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0070 - val_loss: 0.0076\n",
            "Epoch 85/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0068 - val_loss: 0.0062\n",
            "Epoch 86/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0068 - val_loss: 0.0065\n",
            "Epoch 87/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0068 - val_loss: 0.0066\n",
            "Epoch 88/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0070 - val_loss: 0.0067\n",
            "Epoch 89/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0069 - val_loss: 0.0073\n",
            "Epoch 90/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0067 - val_loss: 0.0079\n",
            "Epoch 91/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0068 - val_loss: 0.0072\n",
            "Epoch 92/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0068 - val_loss: 0.0078\n",
            "Epoch 93/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0068 - val_loss: 0.0075\n",
            "Epoch 94/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0069 - val_loss: 0.0061\n",
            "Epoch 95/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0066 - val_loss: 0.0060\n",
            "Epoch 96/200\n",
            "733/733 [==============================] - 3s 3ms/step - loss: 0.0067 - val_loss: 0.0072\n",
            "Epoch 97/200\n",
            "733/733 [==============================] - 3s 3ms/step - loss: 0.0067 - val_loss: 0.0066\n",
            "Epoch 98/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0065 - val_loss: 0.0062\n",
            "Epoch 99/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0066 - val_loss: 0.0052\n",
            "Epoch 100/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0065 - val_loss: 0.0061\n",
            "Epoch 101/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0065 - val_loss: 0.0068\n",
            "Epoch 102/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0067 - val_loss: 0.0069\n",
            "Epoch 103/200\n",
            "733/733 [==============================] - 3s 4ms/step - loss: 0.0065 - val_loss: 0.0061\n",
            "Epoch 104/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0066 - val_loss: 0.0075\n",
            "Epoch 105/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0065 - val_loss: 0.0070\n",
            "Epoch 106/200\n",
            "733/733 [==============================] - 3s 3ms/step - loss: 0.0066 - val_loss: 0.0072\n",
            "Epoch 107/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0064 - val_loss: 0.0069\n",
            "Epoch 108/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0065 - val_loss: 0.0075\n",
            "Epoch 109/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0064 - val_loss: 0.0063\n",
            "Epoch 110/200\n",
            "733/733 [==============================] - 3s 3ms/step - loss: 0.0065 - val_loss: 0.0071\n",
            "Epoch 111/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0065 - val_loss: 0.0070\n",
            "Epoch 112/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0064 - val_loss: 0.0082\n",
            "Epoch 113/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0065 - val_loss: 0.0062\n",
            "Epoch 114/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0063 - val_loss: 0.0062\n",
            "Epoch 115/200\n",
            "733/733 [==============================] - 3s 3ms/step - loss: 0.0064 - val_loss: 0.0070\n",
            "Epoch 116/200\n",
            "733/733 [==============================] - 3s 3ms/step - loss: 0.0064 - val_loss: 0.0078\n",
            "Epoch 117/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0064 - val_loss: 0.0069\n",
            "Epoch 118/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0064 - val_loss: 0.0068\n",
            "Epoch 119/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0063 - val_loss: 0.0063\n",
            "Epoch 120/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0062 - val_loss: 0.0063\n",
            "Epoch 121/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0063 - val_loss: 0.0057\n",
            "Epoch 122/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0062 - val_loss: 0.0065\n",
            "Epoch 123/200\n",
            "733/733 [==============================] - 3s 3ms/step - loss: 0.0063 - val_loss: 0.0064\n",
            "Epoch 124/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0060 - val_loss: 0.0068\n",
            "Epoch 125/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0063 - val_loss: 0.0063\n",
            "Epoch 126/200\n",
            "733/733 [==============================] - 3s 3ms/step - loss: 0.0061 - val_loss: 0.0064\n",
            "Epoch 127/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0062 - val_loss: 0.0059\n",
            "Epoch 128/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0063 - val_loss: 0.0064\n",
            "Epoch 129/200\n",
            "733/733 [==============================] - 3s 3ms/step - loss: 0.0061 - val_loss: 0.0074\n",
            "Epoch 130/200\n",
            "733/733 [==============================] - 3s 3ms/step - loss: 0.0063 - val_loss: 0.0060\n",
            "Epoch 131/200\n",
            "733/733 [==============================] - 3s 3ms/step - loss: 0.0062 - val_loss: 0.0057\n",
            "Epoch 132/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0061 - val_loss: 0.0063\n",
            "Epoch 133/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0062 - val_loss: 0.0063\n",
            "Epoch 134/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0061 - val_loss: 0.0059\n",
            "Epoch 135/200\n",
            "733/733 [==============================] - 3s 3ms/step - loss: 0.0061 - val_loss: 0.0054\n",
            "Epoch 136/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0060 - val_loss: 0.0054\n",
            "Epoch 137/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0062 - val_loss: 0.0059\n",
            "Epoch 138/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0061 - val_loss: 0.0064\n",
            "Epoch 139/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0059 - val_loss: 0.0056\n",
            "Epoch 140/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0060 - val_loss: 0.0068\n",
            "Epoch 141/200\n",
            "733/733 [==============================] - 3s 3ms/step - loss: 0.0061 - val_loss: 0.0072\n",
            "Epoch 142/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0061 - val_loss: 0.0059\n",
            "Epoch 143/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0059 - val_loss: 0.0061\n",
            "Epoch 144/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0060 - val_loss: 0.0053\n",
            "Epoch 145/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0060 - val_loss: 0.0058\n",
            "Epoch 146/200\n",
            "733/733 [==============================] - 3s 4ms/step - loss: 0.0059 - val_loss: 0.0074\n",
            "Epoch 147/200\n",
            "733/733 [==============================] - 3s 4ms/step - loss: 0.0059 - val_loss: 0.0058\n",
            "Epoch 148/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0060 - val_loss: 0.0062\n",
            "Epoch 149/200\n",
            "733/733 [==============================] - 3s 4ms/step - loss: 0.0059 - val_loss: 0.0062\n",
            "Epoch 150/200\n",
            "733/733 [==============================] - 3s 4ms/step - loss: 0.0060 - val_loss: 0.0052\n",
            "Epoch 151/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0058 - val_loss: 0.0059\n",
            "Epoch 152/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0059 - val_loss: 0.0061\n",
            "Epoch 153/200\n",
            "733/733 [==============================] - 3s 3ms/step - loss: 0.0060 - val_loss: 0.0046\n",
            "Epoch 154/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0058 - val_loss: 0.0059\n",
            "Epoch 155/200\n",
            "733/733 [==============================] - 3s 4ms/step - loss: 0.0060 - val_loss: 0.0057\n",
            "Epoch 156/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0058 - val_loss: 0.0059\n",
            "Epoch 157/200\n",
            "733/733 [==============================] - 3s 4ms/step - loss: 0.0057 - val_loss: 0.0055\n",
            "Epoch 158/200\n",
            "733/733 [==============================] - 3s 4ms/step - loss: 0.0058 - val_loss: 0.0061\n",
            "Epoch 159/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0058 - val_loss: 0.0064\n",
            "Epoch 160/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0058 - val_loss: 0.0068\n",
            "Epoch 161/200\n",
            "733/733 [==============================] - 3s 3ms/step - loss: 0.0056 - val_loss: 0.0060\n",
            "Epoch 162/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0056 - val_loss: 0.0057\n",
            "Epoch 163/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0057 - val_loss: 0.0056\n",
            "Epoch 164/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0057 - val_loss: 0.0056\n",
            "Epoch 165/200\n",
            "733/733 [==============================] - 3s 4ms/step - loss: 0.0057 - val_loss: 0.0060\n",
            "Epoch 166/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0056 - val_loss: 0.0061\n",
            "Epoch 167/200\n",
            "733/733 [==============================] - 3s 3ms/step - loss: 0.0057 - val_loss: 0.0055\n",
            "Epoch 168/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0058 - val_loss: 0.0052\n",
            "Epoch 169/200\n",
            "733/733 [==============================] - 3s 4ms/step - loss: 0.0055 - val_loss: 0.0055\n",
            "Epoch 170/200\n",
            "733/733 [==============================] - 3s 4ms/step - loss: 0.0057 - val_loss: 0.0052\n",
            "Epoch 171/200\n",
            "733/733 [==============================] - 3s 4ms/step - loss: 0.0057 - val_loss: 0.0056\n",
            "Epoch 172/200\n",
            "733/733 [==============================] - 3s 3ms/step - loss: 0.0054 - val_loss: 0.0062\n",
            "Epoch 173/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0058 - val_loss: 0.0061\n",
            "Epoch 174/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0056 - val_loss: 0.0068\n",
            "Epoch 175/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0057 - val_loss: 0.0065\n",
            "Epoch 176/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0056 - val_loss: 0.0053\n",
            "Epoch 177/200\n",
            "733/733 [==============================] - 3s 3ms/step - loss: 0.0056 - val_loss: 0.0062\n",
            "Epoch 178/200\n",
            "733/733 [==============================] - 3s 3ms/step - loss: 0.0055 - val_loss: 0.0060\n",
            "Epoch 179/200\n",
            "733/733 [==============================] - 3s 4ms/step - loss: 0.0056 - val_loss: 0.0064\n",
            "Epoch 180/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0057 - val_loss: 0.0060\n",
            "Epoch 181/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0056 - val_loss: 0.0049\n",
            "Epoch 182/200\n",
            "733/733 [==============================] - 3s 3ms/step - loss: 0.0058 - val_loss: 0.0054\n",
            "Epoch 183/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0055 - val_loss: 0.0055\n",
            "Epoch 184/200\n",
            "733/733 [==============================] - 3s 3ms/step - loss: 0.0055 - val_loss: 0.0053\n",
            "Epoch 185/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0054 - val_loss: 0.0067\n",
            "Epoch 186/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0055 - val_loss: 0.0053\n",
            "Epoch 187/200\n",
            "733/733 [==============================] - 3s 4ms/step - loss: 0.0055 - val_loss: 0.0054\n",
            "Epoch 188/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0054 - val_loss: 0.0063\n",
            "Epoch 189/200\n",
            "733/733 [==============================] - 3s 4ms/step - loss: 0.0054 - val_loss: 0.0053\n",
            "Epoch 190/200\n",
            "733/733 [==============================] - 3s 4ms/step - loss: 0.0055 - val_loss: 0.0053\n",
            "Epoch 191/200\n",
            "733/733 [==============================] - 3s 3ms/step - loss: 0.0055 - val_loss: 0.0061\n",
            "Epoch 192/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0054 - val_loss: 0.0055\n",
            "Epoch 193/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0056 - val_loss: 0.0052\n",
            "Epoch 194/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0054 - val_loss: 0.0053\n",
            "Epoch 195/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0053 - val_loss: 0.0054\n",
            "Epoch 196/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0055 - val_loss: 0.0051\n",
            "Epoch 197/200\n",
            "733/733 [==============================] - 3s 3ms/step - loss: 0.0053 - val_loss: 0.0052\n",
            "Epoch 198/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0054 - val_loss: 0.0053\n",
            "Epoch 199/200\n",
            "733/733 [==============================] - 2s 3ms/step - loss: 0.0054 - val_loss: 0.0050\n",
            "Epoch 200/200\n",
            "733/733 [==============================] - 3s 3ms/step - loss: 0.0053 - val_loss: 0.0066\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "17.912362422340667\n",
            "3.3820314331118317\n",
            "4.051213760660136\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5hyTTJtQuNA"
      },
      "source": [
        "import numpy as np\n",
        "from scipy.stats import special_ortho_group\n",
        "\n",
        "X = special_ortho_group.rvs(5)\n",
        "\n",
        "a1 = X[:,0]\n",
        "a2 = X[:,1]\n",
        "a3 = X[:,2]\n",
        "a4 = X[:,3]\n",
        "\n",
        "b1 = a1 / np.linalg.norm(a1)\n",
        "\n",
        "b2 = a2 - np.dot(a1,b1)*b1\n",
        "b2 = b2 / np.linalg.norm(b2)\n",
        "\n",
        "b3 = a3 - (np.dot(a3,b1)*b1) - np.dot(a3,b2)*b2\n",
        "b3 = b3 / np.linalg.norm(b3)\n",
        "\n",
        "b4 = a4 - (np.dot(a4,b1)*b1) - np.dot(a4,b2)*b2 - np.dot(a4,b3)*b3\n",
        "\n",
        "\n",
        "print(n)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yGtKY3xXVjM"
      },
      "source": [
        "print(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzO_EQ2NQ4tj"
      },
      "source": [
        "!zip -r /content/ /content\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"/content/.zip\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
